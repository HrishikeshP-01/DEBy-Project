{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install necessary packages"
      ],
      "metadata": {
        "id": "GYflWs3gRrc3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5u91esq4xBR",
        "outputId": "120f1182-5467-4fdd-ab0f-fbbd228eb78a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nx-arangodb\n",
            "  Downloading nx_arangodb-1.3.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting networkx<=3.4,>=3.0 (from nx-arangodb)\n",
            "  Downloading networkx-3.4-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting phenolrs~=0.5 (from nx-arangodb)\n",
            "  Downloading phenolrs-0.5.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting python-arango~=8.1 (from nx-arangodb)\n",
            "  Downloading python_arango-8.1.6-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting adbnx-adapter~=5.0.5 (from nx-arangodb)\n",
            "  Downloading adbnx_adapter-5.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (2.32.3)\n",
            "Requirement already satisfied: rich>=12.5.1 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (13.9.4)\n",
            "Requirement already satisfied: setuptools>=45 in /usr/local/lib/python3.11/dist-packages (from adbnx-adapter~=5.0.5->nx-arangodb) (75.1.0)\n",
            "Requirement already satisfied: numpy~=1.26 in /usr/local/lib/python3.11/dist-packages (from phenolrs~=0.5->nx-arangodb) (1.26.4)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (2.3.0)\n",
            "Requirement already satisfied: requests_toolbelt in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (1.0.0)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (2.10.1)\n",
            "Requirement already satisfied: importlib_metadata>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (8.6.1)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.11/dist-packages (from python-arango~=8.1->nx-arangodb) (24.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=4.7.1->python-arango~=8.1->nx-arangodb) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->adbnx-adapter~=5.0.5->nx-arangodb) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.5.1->adbnx-adapter~=5.0.5->nx-arangodb) (0.1.2)\n",
            "Downloading nx_arangodb-1.3.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading adbnx_adapter-5.0.6-py3-none-any.whl (21 kB)\n",
            "Downloading networkx-3.4-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading phenolrs-0.5.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_arango-8.1.6-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: networkx, python-arango, phenolrs, adbnx-adapter, nx-arangodb\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.4.2\n",
            "    Uninstalling networkx-3.4.2:\n",
            "      Successfully uninstalled networkx-3.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed adbnx-adapter-5.0.6 networkx-3.4 nx-arangodb-1.3.0 phenolrs-0.5.9 python-arango-8.1.6\n"
          ]
        }
      ],
      "source": [
        "# 1. Install nx-arangodb via pip\n",
        "# Github: https://github.com/arangodb/nx-arangodb\n",
        "\n",
        "!pip install nx-arangodb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Check if you have an NVIDIA GPU\n",
        "# Note: If this returns \"command not found\", then GPU-based algorithms via cuGraph are unavailable\n",
        "\n",
        "!nvidia-smi\n",
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZjsnsN6CStZ",
        "outputId": "ff635099-bade-4ce6-94a6-66802767f680"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Install nx-cugraph via pip\n",
        "# Note: Only enable this installation if the step above is working!\n",
        "\n",
        "!pip install nx-cugraph-cu12 --extra-index-url https://pypi.nvidia.com # Requires CUDA-capable GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DlkxNsSaCZmQ",
        "outputId": "ebda1be4-b5a5-4784-97bc-48e497961aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
            "Requirement already satisfied: nx-cugraph-cu12 in /usr/local/lib/python3.11/dist-packages (24.12.0)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (13.3.0)\n",
            "Requirement already satisfied: networkx>=3.2 in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (3.4)\n",
            "Requirement already satisfied: numpy<3.0a0,>=1.23 in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (1.26.4)\n",
            "Requirement already satisfied: pylibcugraph-cu12==24.12.* in /usr/local/lib/python3.11/dist-packages (from nx-cugraph-cu12) (24.12.0)\n",
            "Requirement already satisfied: nvidia-cublas-cu12 in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (12.5.3.2)\n",
            "Requirement already satisfied: nvidia-curand-cu12 in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (10.3.6.82)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12 in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (11.6.3.83)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12 in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (12.5.1.3)\n",
            "Requirement already satisfied: pylibraft-cu12==24.12.* in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (24.12.0)\n",
            "Requirement already satisfied: rmm-cu12==24.12.* in /usr/local/lib/python3.11/dist-packages (from pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (24.12.1)\n",
            "Requirement already satisfied: cuda-python<13.0a0,<=12.6.0,>=12.0 in /usr/local/lib/python3.11/dist-packages (from pylibraft-cu12==24.12.*->pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (12.6.0)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.11/dist-packages (from rmm-cu12==24.12.*->pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (0.61.0)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x>=12.0.0->nx-cugraph-cu12) (0.8.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12->pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (12.5.82)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57->rmm-cu12==24.12.*->pylibcugraph-cu12==24.12.*->nx-cugraph-cu12) (0.44.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Install LangChain & LangGraph\n",
        "\n",
        "!pip install --upgrade langchain langchain-community langchain-openai langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-P4NSlrCfEs",
        "outputId": "44d059b0-7bfe-4fef-c2e4-9101f5a6fde6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.58.1 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.61.1)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.16-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.1 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.53-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.7-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.3.2-py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.9/130.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langgraph_checkpoint-2.0.16-py3-none-any.whl (38 kB)\n",
            "Downloading langgraph_prebuilt-0.1.1-py3-none-any.whl (24 kB)\n",
            "Downloading langgraph_sdk-0.1.53-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, tiktoken, pydantic-settings, langgraph-sdk, dataclasses-json, langgraph-checkpoint, langchain-openai, langgraph-prebuilt, langgraph, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.18 langchain-openai-0.3.7 langgraph-0.3.2 langgraph-checkpoint-2.0.16 langgraph-prebuilt-0.1.1 langgraph-sdk-0.1.53 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 python-dotenv-1.0.1 tiktoken-0.9.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyArango\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9f9wm4rHun7",
        "outputId": "2f631d61-3246-4ad3-8308-a2de4a060331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyArango\n",
            "  Downloading pyArango-2.1.1.tar.gz (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from pyArango) (2.32.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from pyArango) (1.0.0)\n",
            "Collecting datetime (from pyArango)\n",
            "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->pyArango) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->pyArango) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->pyArango) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->pyArango) (2025.1.31)\n",
            "Collecting zope.interface (from datetime->pyArango)\n",
            "  Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from datetime->pyArango) (2025.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from zope.interface->datetime->pyArango) (75.1.0)\n",
            "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.8/259.8 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyArango\n",
            "  Building wheel for pyArango (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyArango: filename=pyArango-2.1.1-py2.py3-none-any.whl size=53233 sha256=b07cf16ca2ea8477977c73accb9b7c1d0010f9a7cf0b93ac88bba4f91310baca\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/7e/13/19e5408e459cb347796f7a51b4394976d8107a1cb68a060892\n",
            "Successfully built pyArango\n",
            "Installing collected packages: zope.interface, datetime, pyArango\n",
            "Successfully installed datetime-5.5 pyArango-2.1.1 zope.interface-7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required modules"
      ],
      "metadata": {
        "id": "rPm3NhEgR3V0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Import the required modules\n",
        "\n",
        "import networkx as nx\n",
        "import nx_arangodb as nxadb\n",
        "\n",
        "from arango import ArangoClient\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from random import randint\n",
        "import re\n",
        "\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.graphs import ArangoGraph\n",
        "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
        "#from langchain_core.tools import tool\n",
        "from langchain.agents import tool\n",
        "from langchain_core.messages import AIMessage"
      ],
      "metadata": {
        "id": "81QDdUDtCjp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langchain imports\n",
        "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
        "from langchain.prompts import BaseChatPromptTemplate, ChatPromptTemplate\n",
        "from langchain import LLMChain\n",
        "from langchain.schema import AgentAction, AgentFinish, HumanMessage, SystemMessage\n",
        "# LLM wrapper\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import OpenAI\n",
        "# Conversational memory\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "# Embeddings and vectorstore\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "hfc9XtaIGlxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to ArangoDB"
      ],
      "metadata": {
        "id": "X2tEecZFSU7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the ArangoDB database\n",
        "# Configure your credentials here\n",
        "\n",
        "db = ArangoClient(hosts=\"https://9cc4f92f2a8b.arangodb.cloud:8529\").db(\"DebyDB\", username=\"root\", password=\"\", verify=True)\n",
        "\n",
        "print(db)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dr8mqPkzClzd",
        "outputId": "51e4edd8-c6d8-4545-b90c-b6428b08e36e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<StandardDatabase DebyDB>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Re-connect to the same Graph\n",
        "\n",
        "G_adb = nxadb.DiGraph(name=\"DependencyGraph\", db=db)\n",
        "\n",
        "print(G_adb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o4hfG3SDJcP",
        "outputId": "180fe6a2-46b7-475c-d5b3-d1e4a3ec0b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[12:05:00 +0000] [INFO]: Graph 'DependencyGraph' exists.\n",
            "INFO:nx_arangodb:Graph 'DependencyGraph' exists.\n",
            "[12:05:00 +0000] [INFO]: Default node type set to 'Sink'\n",
            "INFO:nx_arangodb:Default node type set to 'Sink'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiGraph named 'DependencyGraph' with 58 nodes and 56 edges\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Print the degree of a Node\n",
        "\n",
        "G_adb.degree('ECOM.v_OrdersWithShippingInfo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yeW_OoaDOCx",
        "outputId": "9df50b9e-e48b-47d1-826a-e79ba80349e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create the ArangoGraph LangChain wrapper\n",
        "# Reference: https://api.python.langchain.com/en/latest/graphs/langchain_community.graphs.arangodb_graph.ArangoGraph.html\n",
        "\n",
        "arango_graph = ArangoGraph(db)"
      ],
      "metadata": {
        "id": "QxKFxN6eDXHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create tools for AI agent"
      ],
      "metadata": {
        "id": "krTZJJDHSbTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define the llm object\n",
        "# Note: You can use any llm you want. We will be using OpenAI for example purposes.\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"Enter API here\"\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n"
      ],
      "metadata": {
        "id": "dcYNSJxbDYT2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c206884a-5355-4faa-a9b2-a396ee50d7a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-82137750383d>:8: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple tool to provide an introduction\n",
        "@tool\n",
        "def who_am_i(query: str):\n",
        "    \"\"\"Your name is DEBy. You are a data engineer's assistant. You help with tasks like intelligent data lineage tracking, query forumulation, data retrieval and much more.\n",
        "    Whenever asked who you are you must respond with an answer that takes the above text into account.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "LNi06SHXMv8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Limit responses to those that are relavent to data & data engineering\n",
        "@tool\n",
        "def out_of_scope(query: str):\n",
        "    \"\"\"Any question that is not related to data or data engineering should be answered with this tool.\n",
        "    Let the user know who you are. Then tell them that the question is irrelavent to your job function. Let them know that you are here to help with data related queries.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "Pq-16qgCNxdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define the Text to AQL Tool\n",
        "# Reference: https://python.langchain.com/docs/integrations/graphs/arangodb/\n",
        "# Reference: https://python.langchain.com/api_reference/community/chains/langchain_community.chains.graph_qa.arangodb.ArangoGraphQAChain.html\n",
        "# Note: It is encouraged to experiment and improve this section! This is just a placeholder:\n",
        "\n",
        "@tool\n",
        "def text_to_aql_to_text(query: str):\n",
        "    \"\"\"This tool is available to invoke the\n",
        "    ArangoGraphQAChain object, which enables you to\n",
        "    translate a Natural Language Query into AQL, execute\n",
        "    the query, and translate the result back into Natural Language.\n",
        "    Never use this to find upstream or downstream dependencies\n",
        "    \"\"\"\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "    chain = ArangoGraphQAChain.from_llm(\n",
        "    \tllm=llm,\n",
        "    \tgraph=arango_graph,\n",
        "    \tverbose=True,\n",
        "        allow_dangerous_requests=True\n",
        "    )\n",
        "\n",
        "    result = chain.invoke(query)\n",
        "\n",
        "    return str(result[\"result\"])"
      ],
      "metadata": {
        "id": "gi7RSkb-Df0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use AQL to find the immediate parents of an SQL object\n",
        "@tool\n",
        "def get_source_objects(query: str):\n",
        "  \"\"\" Use this tool to get the source objects of an SQL object like a table or view.\n",
        "  \"\"\"\n",
        "  return text_to_aql_to_text(query+\"Find the parents of the node\")"
      ],
      "metadata": {
        "id": "aSRrneceoPVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use NetworkX to find the downstream dependencies\n",
        "# Here I'm providing detailed instructions so that the llm doesn't hallucinate & look other data cataloging & lineage resources\n",
        "@tool\n",
        "def get_sink_tree(query: str):\n",
        "  \"\"\" Use this tool to get the sink tree of an SQL object like a table or view\n",
        "  It can be used to trace the downstream dependencies of the SQL object\n",
        "  It can be used to determine which objects might be impacted due to any modifications in its definition\n",
        "  \"\"\"\n",
        "  q1 = text_to_nx_algorithm_to_text(query+\"\"\"Follow the following steps to find the sink tree:\n",
        "  1. If you are trying this tool print trying get_sink_tree\n",
        "  2. Get the name of the SQL object from the query\n",
        "  3. Find the node from the graph that has the same or similar name so use this code:\n",
        "  target_node = None\n",
        "  for node, data in G_adb.nodes(data=True):\n",
        "    if data.get('_key') == sql_object_name:\n",
        "        target_node = node\n",
        "        break\n",
        "  4. Convert ArangoDB graph to NetwrokX graph like so:\n",
        "  nx_graph = nx.DiGraph()  # Create an empty NetworkX DiGraph\n",
        "  for source, target, data in G_adb.edges():\n",
        "    nx_graph.add_edge(source, target)\n",
        "  5. Find all the nodes that are reachable from this node in the graph. This is a directed graph so use this python code:\n",
        "  descendants = set(nx.descendants(nx_graph, target_node))\n",
        "  ancestors = set(nx.ancestors(nx_graph, target_node))\n",
        "  sinks = list(descendants - ancestors)\n",
        "  6. Analyse the answer & only return the SQL objects that are consuming data from the SQL object provided by the user\n",
        "  \"\"\")\n",
        "  llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "  prompt = q1 + 'Analyse the answer & return only the list of SQL objects that are consuming data from the SQL object provided by the user'\n",
        "  response = llm.invoke(prompt)\n",
        "  if isinstance(response, AIMessage):\n",
        "    return response.content\n",
        "  else:\n",
        "    return str(response)"
      ],
      "metadata": {
        "id": "0rD_E9WAPR4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use NetworkX to find the upsteam dependencies\n",
        "# I'm providing detailed instructions & sample code snippets to prevent the llms from making assumptions abt the nature of relationships\n",
        "@tool\n",
        "def get_source_tree(query: str):\n",
        "  \"\"\" Use this tool to get the source tree of an SQL object like a table or view\n",
        "  It can be used to trace the upstream dependencies of the SQL object\n",
        "  It can be used to determine which objects provide data to the object mentioned by the user\n",
        "  \"\"\"\n",
        "  q1 = text_to_nx_algorithm_to_text(query+\"\"\"Follow the following steps to find the sink tree:\n",
        "  1. If you are trying this tool print trying get_source_tree\n",
        "  2. Get the name of the SQL object from the query\n",
        "  3. Find the node from the graph that has the same or similar name so use this code:\n",
        "  target_node = None\n",
        "  for node, data in G_adb.nodes(data=True):\n",
        "    if data.get('_key') == sql_object_name:\n",
        "        target_node = node\n",
        "        break\n",
        "  4. Make sure to created a NetworkX DiGraph using the nx_graph.add_edge_from() function:\n",
        "  nx_graph = nx.DiGraph()  # Create an empty NetworkX DiGraph\n",
        "  for source, target, data in G_adb.edges():\n",
        "    nx_graph.add_edge_from(source, target)\n",
        "  5. Find all the nodes that are reachable from this node in the graph. This is a directed graph so use this python code:\n",
        "  ancestors = list(nx.ancestors(nx_graph, target_node))\n",
        "  \"\"\")\n",
        "  llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "  prompt = q1 + 'Analyse the answer & return only the list of SQL objects that are providing data to the SQL object provided by the user. The first part of the names i.e, Sink/, Source/ are related to the graph not names of the objects so omit them.'\n",
        "  response = llm.invoke(prompt)\n",
        "  if isinstance(response, AIMessage):\n",
        "    return response.content\n",
        "  else:\n",
        "    return str(response)"
      ],
      "metadata": {
        "id": "ZI1bUFsCjjxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Define the Text to NetworkX/cuGraph Tool\n",
        "# Made slight modifications to use DiGraph\n",
        "\n",
        "@tool\n",
        "def text_to_nx_algorithm_to_text(query):\n",
        "    \"\"\"This tool is available to invoke a NetworkX Algorithm on\n",
        "    the ArangoDB DiGraph. You are responsible for accepting the\n",
        "    Natural Language Query, establishing which algorithm needs to\n",
        "    be executed, executing the algorithm, and translating the results back\n",
        "    to Natural Language, with respect to the original query.\n",
        "\n",
        "    If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n",
        "    this tool.\n",
        "    \"\"\"\n",
        "\n",
        "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "\n",
        "    ######################\n",
        "    print(\"1) Generating NetworkX code\")\n",
        "\n",
        "    text_to_nx = llm.invoke(f\"\"\"\n",
        "    I have a NetworkX DiGraph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
        "\n",
        "    I have the following graph analysis query: {query}.\n",
        "\n",
        "    Generate the Python Code required to answer the query using the `G_adb` object.\n",
        "\n",
        "    Be very precise on the NetworkX algorithm you select to answer this query. Think step by step.\n",
        "\n",
        "    Only assume that networkx is installed, and other base python dependencies.\n",
        "\n",
        "    Always set the last variable as `FINAL_RESULT`, which represents the answer to the original query.\n",
        "\n",
        "    Only provide python code that I can directly execute via `exec()`. Do not provide any instructions.\n",
        "\n",
        "    Make sure that `FINAL_RESULT` stores a short & consice answer. Avoid setting this variable to a long sequence.\n",
        "\n",
        "    Your code:\n",
        "    \"\"\").content\n",
        "\n",
        "    text_to_nx_cleaned = re.sub(r\"^```python\\n|```$\", \"\", text_to_nx, flags=re.MULTILINE).strip()\n",
        "\n",
        "    print('-'*10)\n",
        "    print(text_to_nx_cleaned)\n",
        "    print('-'*10)\n",
        "\n",
        "    ######################\n",
        "\n",
        "    print(\"\\n2) Executing NetworkX code\")\n",
        "    global_vars = {\"G_adb\": G_adb, \"nx\": nx}\n",
        "    local_vars = {}\n",
        "\n",
        "    try:\n",
        "        exec(text_to_nx_cleaned, global_vars, local_vars)\n",
        "        text_to_nx_final = text_to_nx\n",
        "    except Exception as e:\n",
        "        print(f\"EXEC ERROR: {e}\")\n",
        "        return f\"EXEC ERROR: {e}\"\n",
        "\n",
        "        # TODO: Consider experimenting with a code corrector!\n",
        "        attempt = 1\n",
        "        MAX_ATTEMPTS = 3\n",
        "\n",
        "        # while attempt <= MAX_ATTEMPTS\n",
        "            # ...\n",
        "\n",
        "    print('-'*10)\n",
        "    FINAL_RESULT = local_vars[\"FINAL_RESULT\"]\n",
        "    print(f\"FINAL_RESULT: {FINAL_RESULT}\")\n",
        "    print('-'*10)\n",
        "\n",
        "    ######################\n",
        "\n",
        "    print(\"3) Formulating final answer\")\n",
        "\n",
        "    nx_to_text = llm.invoke(f\"\"\"\n",
        "        I have a NetworkX DiGraph called `G_adb`. It has the following schema: {arango_graph.schema}\n",
        "\n",
        "        I have the following graph analysis query: {query}.\n",
        "\n",
        "        I have executed the following python code to help me answer my query:\n",
        "\n",
        "        ---\n",
        "        {text_to_nx_final}\n",
        "        ---\n",
        "\n",
        "        The `FINAL_RESULT` variable is set to the following: {FINAL_RESULT}.\n",
        "\n",
        "        Based on my original Query and FINAL_RESULT, generate a short and concise response to\n",
        "        answer my query.\n",
        "\n",
        "        Your response:\n",
        "    \"\"\").content\n",
        "\n",
        "    return nx_to_text"
      ],
      "metadata": {
        "id": "mayyQkp-A_mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple tool to query the AQL documents in my DB & get the refresh time metadata\n",
        "# Returns the scheduled time to refresh of a table\n",
        "from pyArango.connection import *\n",
        "@tool\n",
        "def get_table_refreshTime(query):\n",
        "  \"\"\"\n",
        "  Tool to find the refresh time of a table\n",
        "  \"\"\"\n",
        "# Connect to ArangoDB\n",
        "  conn = Connection(arangoURL=\"https://9cc4f92f2a8b.arangodb.cloud:8529\", username=\"root\", password=\"\")\n",
        "  db=conn[\"DebyDB\"]\n",
        "\n",
        "# Define the collection you want to query\n",
        "  collection_name = \"RefreshDetails\"\n",
        "\n",
        "  llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\")\n",
        "  prompt = \"\"\"Find the schema & table name. the answer in the format schema.table name nothing else\n",
        "  If the input is a string with no spaces, return that as the answer\"\"\"\n",
        "  prompt = query + prompt\n",
        "  response = llm.invoke(prompt)\n",
        "  if isinstance(response, AIMessage):\n",
        "    value= response.content\n",
        "  else:\n",
        "    value= str(response)\n",
        "  print(value)\n",
        "\n",
        "# Define the document key you want to query\n",
        "  document_key = \"your_document_key\"\n",
        "  keyname='TableName'\n",
        "  aql_query = f'''\n",
        "  FOR doc IN {collection_name}\n",
        "      FILTER doc.{keyname} == \"{value}\"\n",
        "      RETURN doc\n",
        "  '''\n",
        "\n",
        "# Use db.query() instead of db.AQLQuery()\n",
        "  cursor = db.AQLQuery(aql_query, rawResults=True)  # Execute the AQL query\n",
        "\n",
        "  # Print the result\n",
        "  for doc in cursor:  # Iterate through the cursor to get results\n",
        "      return(doc['Time'])"
      ],
      "metadata": {
        "id": "KavWAjRfN9kg"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_table_refreshTime('ECOM.v_UserFullName')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "U-Ga4CWqAICj",
        "outputId": "cb9e72fe-3138-4ecb-8be4-911e91312c21"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ECOM.v_UserFullName\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2:26 AM'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name='get_source_objects',\n",
        "        func=get_source_objects,\n",
        "        description='Use this tool to get the source objects of an SQL object like a table or view.'\n",
        "    ),\n",
        "    Tool(\n",
        "        name='get_sink_tree',\n",
        "        func=get_sink_tree,\n",
        "        description='Use this tool to find the sink tree of an SQL object like a table or view. This can also be used to find the downstream dependencies. Can be used to find the objects impacted by any modifications to the object mentioned by the user'\n",
        "    ),\n",
        "    Tool(\n",
        "        name='get_source_tree',\n",
        "        func=get_source_tree,\n",
        "        description='Use this tool to find the sink tree of an SQL object like a table or view. This can also be used to find the upstream dependencies. Can be used to find the objects that provide data to the object mentioned by the user'\n",
        "    ),\n",
        "    Tool(\n",
        "        name='who_am_i',\n",
        "        func=who_am_i,\n",
        "        description='Use this tool to talk about yourself'\n",
        "    ),\n",
        "    Tool(\n",
        "        name='text_to_aql_to_text',\n",
        "        func=text_to_aql_to_text,\n",
        "        description=\"\"\"This tool is available to invoke the\n",
        "    ArangoGraphQAChain object, which enables you to\n",
        "    translate a Natural Language Query into AQL, execute\n",
        "    the query, and translate the result back into Natural Language.\n",
        "    Never use this tool for tasks involving upstream or downstream dependencies\n",
        "    \"\"\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name='text_to_nx_algorithm_to_text',\n",
        "        func=text_to_nx_algorithm_to_text,\n",
        "        description=\"\"\"This tool is available to invoke a NetworkX Algorithm on\n",
        "    the ArangoDB DiGraph. You are responsible for accepting the\n",
        "    Natural Language Query, establishing which algorithm needs to\n",
        "    be executed, executing the algorithm, and translating the results back\n",
        "    to Natural Language, with respect to the original query.\n",
        "\n",
        "    If the query (e.g traversals, shortest path, etc.) can be solved using the Arango Query Language, then do not use\n",
        "    this tool.\n",
        "    \"\"\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name='get_table_refreshTime',\n",
        "        func=get_table_refreshTime,\n",
        "        description=\"\"\"To find the refresh time of a table\n",
        "        \"\"\"\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "3kVVX5m9IrqN"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up prompt templates"
      ],
      "metadata": {
        "id": "RS6JOPsBT8_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a prompt template which can interpolate the history\n",
        "template_with_history = \"\"\"You are DEBy, an intelligent Data Engineer's assistant who provides informative answers to users regarding data & data engineering.\n",
        "Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format:\n",
        "\n",
        "Question: the input question you must answer\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [{tool_names}]\n",
        "Action Input: the input to the action\n",
        "Observation: the result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought: I now know the final answer\n",
        "Final Answer: the final answer to the original input question\n",
        "\n",
        "Begin! Remember to give detailed, informative answers\n",
        "\n",
        "Previous conversation history:\n",
        "{history}\n",
        "\n",
        "New question: {input}\n",
        "{agent_scratchpad}\"\"\""
      ],
      "metadata": {
        "id": "YIcE8wyEGt0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "from langchain.tools import BaseTool\n",
        "\n",
        "# Set up a prompt template\n",
        "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
        "    # The template to use\n",
        "    template: str\n",
        "    # The list of tools available\n",
        "    tools: List[Tool]\n",
        "\n",
        "    def format_messages(self, **kwargs) -> str:\n",
        "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
        "\n",
        "        # Format them in a particular way\n",
        "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
        "        thoughts = \"\"\n",
        "        for action, observation in intermediate_steps:\n",
        "            thoughts += action.log\n",
        "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
        "\n",
        "        # Set the agent_scratchpad variable to that value\n",
        "        kwargs[\"agent_scratchpad\"] = thoughts\n",
        "\n",
        "        # Create a tools variable from the list of tools provided\n",
        "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
        "\n",
        "        # Create a list of tool names for the tools provided\n",
        "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
        "        formatted = self.template.format(**kwargs)\n",
        "        return [HumanMessage(content=formatted)]\n",
        "\n",
        "prompt = CustomPromptTemplate(\n",
        "    template=template_with_history,\n",
        "    tools=tools,\n",
        "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
        "    # This includes the `intermediate_steps` variable because that is needed\n",
        "    input_variables=[\"input\", \"intermediate_steps\"]\n",
        ")"
      ],
      "metadata": {
        "id": "LexqlzahHsbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomOutputParser(AgentOutputParser):\n",
        "\n",
        "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
        "\n",
        "        # Check if agent should finish\n",
        "        if \"Final Answer:\" in llm_output:\n",
        "            return AgentFinish(\n",
        "                # Return values is generally always a dictionary with a single `output` key\n",
        "                # It is not recommended to try anything else at the moment :)\n",
        "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "\n",
        "        # Check if the output contains the answer without following the expected format.\n",
        "        # This is an adjustment to handle the conversational response.\n",
        "        if \"I now know the final answer.\" in llm_output:\n",
        "            return AgentFinish(\n",
        "                return_values={\"output\": llm_output.split(\"I now know the final answer.\")[-1].strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "\n",
        "\n",
        "        # Parse out the action and action input\n",
        "        regex = r\"Action: (.*?)[\\n]*Action Input:[\\s]*(.*)\"\n",
        "        match = re.search(regex, llm_output, re.DOTALL)\n",
        "\n",
        "        # If it can't parse the output it raises an error\n",
        "        # You can add your own logic here to handle errors in a different way i.e. pass to a human, give a canned response\n",
        "        if 'error' in llm_output: #not match:\n",
        "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
        "        action = None\n",
        "        action_input=None\n",
        "        if match != None:\n",
        "          action = match.group(1).strip()\n",
        "          action_input = match.group(2)\n",
        "\n",
        "        if action == None or action_input==None:\n",
        "          return AgentFinish(\n",
        "                return_values={\"output\": llm_output.strip()},\n",
        "                log=llm_output,\n",
        "            )\n",
        "\n",
        "        # Return the action and action input\n",
        "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)\n",
        "\n",
        "output_parser = CustomOutputParser()"
      ],
      "metadata": {
        "id": "VnXRLgtTqeAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up the agent"
      ],
      "metadata": {
        "id": "roEyWjHrUNsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_with_history = CustomPromptTemplate(\n",
        "    template=template_with_history,\n",
        "    tools=tools,\n",
        "    # The history template includes \"history\" as an input variable so we can interpolate it into the prompt\n",
        "    input_variables=[\"input\", \"intermediate_steps\", \"history\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt_with_history)\n",
        "tool_names = [tool.name for tool in tools]\n",
        "agent = LLMSingleActionAgent(\n",
        "    llm_chain=llm_chain,\n",
        "    output_parser=output_parser,\n",
        "    stop=[\"\\nObservation:\"],\n",
        "    allowed_tools=tool_names\n",
        ")"
      ],
      "metadata": {
        "id": "bfk_-C2RHccY"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate the memory with k=2 to keep the last two turns\n",
        "# Provide the memory to the agent\n",
        "memory = ConversationBufferWindowMemory(k=2)\n",
        "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True, memory=memory)"
      ],
      "metadata": {
        "id": "D9orj-E6NVrR"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q&A with DEBy"
      ],
      "metadata": {
        "id": "A3ze3nnuUSjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple question - Introduce yourself\n",
        "agent_executor.run(\"Who are you\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "4MfRSGZfpA5s",
        "outputId": "502cf563-64ad-4cbf-c180-a0c65ce5133b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The user is asking about my identity and capabilities, so I should use the \"who_am_i\" tool to provide an appropriate response.\n",
            "Action: who_am_i\n",
            "Action Input: None\u001b[0m\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3mNone\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mFinal Answer: I am DEBy, an intelligent Data Engineer's assistant. I am designed to provide informative answers to users regarding data and data engineering. I can help you understand data dependencies, find upstream and downstream data sources, and provide insights into data structures and refresh times. If you have any questions related to data engineering, feel free to ask!\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I am DEBy, an intelligent Data Engineer's assistant. I am designed to provide informative answers to users regarding data and data engineering. I can help you understand data dependencies, find upstream and downstream data sources, and provide insights into data structures and refresh times. If you have any questions related to data engineering, feel free to ask!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question to make the agent generate an AQL script\n",
        "agent_executor.run(\"Find the source of ECOM.v_PaymentsWithOrderInfo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "9GdlilANNXkl",
        "outputId": "c0cd88a9-7111-439f-8d88-e77d9b6770b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo find the source of the view `ECOM.v_PaymentsWithOrderInfo`, I will identify the source objects that provide data to this view.\n",
            "\n",
            "Action: get_source_objects\n",
            "Action Input: ECOM.v_PaymentsWithOrderInfo\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new ArangoGraphQAChain chain...\u001b[0m\n",
            "AQL Query (1):\u001b[32;1m\u001b[1;3m\n",
            "WITH Sink, Mapping, Source\n",
            "FOR v, e, p IN 1..1 INBOUND 'Sink/ECOM.v_PaymentsWithOrderInfo' Mapping\n",
            "RETURN v\n",
            "\u001b[0m\n",
            "AQL Result:\n",
            "\u001b[32;1m\u001b[1;3m[{'_key': 'ECOM.Orders', '_id': 'Source/ECOM.Orders', '_rev': '_jSQBW3i--B'}, {'_key': 'ECOM.Payments', '_id': 'Source/ECOM.Payments', '_rev': '_jSQBW3i--D'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3mThe node 'ECOM.v_PaymentsWithOrderInfo' has two parent nodes. These parent nodes are 'ECOM.Orders' and 'ECOM.Payments'.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe view `ECOM.v_PaymentsWithOrderInfo` is sourced from two parent tables: `ECOM.Orders` and `ECOM.Payments`. These tables provide the data that is used to construct the view.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The view `ECOM.v_PaymentsWithOrderInfo` is sourced from two parent tables: `ECOM.Orders` and `ECOM.Payments`. These tables provide the data that is used to construct the view.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question to find the downstream dependencies of an object\n",
        "agent_executor.run(\"Find the sink tree of ECOM.v_PaymentsWithOrderInfo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 923
        },
        "id": "t4hy7zYndLw7",
        "outputId": "6824c924-20ac-4882-b86f-4415e8021a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mSure, let me find the sink tree for the view `ECOM.v_PaymentsWithOrderInfo` to identify its downstream dependencies.\n",
            "\n",
            "Action: get_sink_tree\n",
            "Action Input: ECOM.v_PaymentsWithOrderInfo\u001b[0m1) Generating NetworkX code\n",
            "----------\n",
            "import networkx as nx\n",
            "\n",
            "# Step 1: Print trying get_sink_tree\n",
            "print(\"trying get_sink_tree\")\n",
            "\n",
            "# Step 2: Get the name of the SQL object from the query\n",
            "sql_object_name = \"ECOM.v_PaymentsWithOrderInfo\"\n",
            "\n",
            "# Step 3: Find the node from the graph that has the same or similar name\n",
            "target_node = None\n",
            "for node, data in G_adb.nodes(data=True):\n",
            "    if data.get('_key') == sql_object_name:\n",
            "        target_node = node\n",
            "        break\n",
            "\n",
            "# Step 4: Convert ArangoDB graph to NetworkX graph\n",
            "nx_graph = nx.DiGraph()  # Create an empty NetworkX DiGraph\n",
            "for source, target, data in G_adb.edges(data=True):\n",
            "    nx_graph.add_edge(source, target)\n",
            "\n",
            "# Step 5: Find all the nodes that are reachable from this node in the graph\n",
            "descendants = set(nx.descendants(nx_graph, target_node))\n",
            "ancestors = set(nx.ancestors(nx_graph, target_node))\n",
            "sinks = list(descendants - ancestors)\n",
            "\n",
            "# Step 6: Analyse the answer & only return the SQL objects that are consuming data from the SQL object provided by the user\n",
            "FINAL_RESULT = [G_adb.nodes[sink]['_key'] for sink in sinks if '_key' in G_adb.nodes[sink]]\n",
            "----------\n",
            "\n",
            "2) Executing NetworkX code\n",
            "trying get_sink_tree\n",
            "----------\n",
            "FINAL_RESULT: []\n",
            "----------\n",
            "3) Formulating final answer\n",
            "\n",
            "\n",
            "Observation:\u001b[33;1m\u001b[1;3mThe analysis indicates that there are no SQL objects consuming data from \"ECOM.v_PaymentsWithOrderInfo.\" Therefore, the list of SQL objects that are consuming data from it is empty.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe view `ECOM.v_PaymentsWithOrderInfo` does not have any downstream dependencies, meaning there are no SQL objects that consume data from this view.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The view `ECOM.v_PaymentsWithOrderInfo` does not have any downstream dependencies, meaning there are no SQL objects that consume data from this view.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Same question, see how the agent retains the memory\n",
        "agent_executor.run(\"Find the downstream dependencies of ECOM.v_PaymentsWithOrderInfo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "wmd4waNjjPO0",
        "outputId": "2a5dcdd5-0921-4103-bb49-52b028798724"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe view `ECOM.v_PaymentsWithOrderInfo` does not have any downstream dependencies. This means there are no SQL objects that consume data from this view.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The view `ECOM.v_PaymentsWithOrderInfo` does not have any downstream dependencies. This means there are no SQL objects that consume data from this view.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the upstream dependencies. Should generate a NetworkX graph\n",
        "agent_executor.run(\"Find the upsteam dependencies of ECOM.v_PaymentsWithOrderInfo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "GUNxjxoFntij",
        "outputId": "ff3d52a4-9d75-4a23-897d-4494134965c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mSure, let's find the upstream dependencies of the view `ECOM.v_PaymentsWithOrderInfo`. This will help us understand which SQL objects provide data to this view.\n",
            "\n",
            "Action: get_source_tree\n",
            "Action Input: ECOM.v_PaymentsWithOrderInfo\u001b[0m1) Generating NetworkX code\n",
            "----------\n",
            "import networkx as nx\n",
            "\n",
            "# Step 1: Print trying get_source_tree\n",
            "print(\"trying get_source_tree\")\n",
            "\n",
            "# Step 2: Get the name of the SQL object from the query\n",
            "sql_object_name = \"ECOM.v_PaymentsWithOrderInfo\"\n",
            "\n",
            "# Step 3: Find the node from the graph that has the same or similar name\n",
            "target_node = None\n",
            "for node, data in G_adb.nodes(data=True):\n",
            "    if data.get('_key') == sql_object_name:\n",
            "        target_node = node\n",
            "        break\n",
            "\n",
            "# Step 4: Create a NetworkX DiGraph using the nx_graph.add_edge_from() function\n",
            "nx_graph = nx.DiGraph()  # Create an empty NetworkX DiGraph\n",
            "for source, target, data in G_adb.edges(data=True):\n",
            "    nx_graph.add_edge(source, target)\n",
            "\n",
            "# Step 5: Find all the nodes that are reachable from this node in the graph\n",
            "ancestors = list(nx.ancestors(nx_graph, target_node))\n",
            "\n",
            "# Set the final result\n",
            "FINAL_RESULT = ancestors\n",
            "----------\n",
            "\n",
            "2) Executing NetworkX code\n",
            "trying get_source_tree\n",
            "----------\n",
            "FINAL_RESULT: ['Source/ECOM.Orders', 'Source/ECOM.Payments']\n",
            "----------\n",
            "3) Formulating final answer\n",
            "\n",
            "\n",
            "Observation:\u001b[38;5;200m\u001b[1;3mThe SQL objects providing data to the SQL object `ECOM.v_PaymentsWithOrderInfo` are:\n",
            "\n",
            "- ECOM.Orders\n",
            "- ECOM.Payments\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe upstream dependencies of the view `ECOM.v_PaymentsWithOrderInfo` are the tables `ECOM.Orders` and `ECOM.Payments`. This means that the data for this view is sourced from these two tables.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The upstream dependencies of the view `ECOM.v_PaymentsWithOrderInfo` are the tables `ECOM.Orders` and `ECOM.Payments`. This means that the data for this view is sourced from these two tables.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AQL + NetworkX\n",
        "# First the Agent should find the parents using AQL then get upstream dependencies using NetworkX\n",
        "agent_executor.run(\"Find the upsteam dependencies of the parent tables of ECOM.v_PaymentsWithOrderInfo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iENMAxdXoNdD",
        "outputId": "f7fef4cc-e3c7-4475-8b13-7e2d1779de5d"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo find the upstream dependencies of the parent tables of `ECOM.v_PaymentsWithOrderInfo`, we need to first identify the parent tables of this view and then trace their upstream dependencies.\n",
            "\n",
            "Let's start by identifying the parent tables of `ECOM.v_PaymentsWithOrderInfo`.\n",
            "\n",
            "Thought: I will use the `get_source_objects` tool to find the parent tables of `ECOM.v_PaymentsWithOrderInfo`.\n",
            "Action: get_source_objects\n",
            "Action Input: ECOM.v_PaymentsWithOrderInfo\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new ArangoGraphQAChain chain...\u001b[0m\n",
            "AQL Query (1):\u001b[32;1m\u001b[1;3m\n",
            "WITH Sink, Mapping, Source\n",
            "FOR vertex, edge, path IN 1..1 INBOUND 'Sink/ECOM.v_PaymentsWithOrderInfo' Mapping\n",
            "RETURN vertex\n",
            "\u001b[0m\n",
            "AQL Result:\n",
            "\u001b[32;1m\u001b[1;3m[{'_key': 'ECOM.Orders', '_id': 'Source/ECOM.Orders', '_rev': '_jSQBW3i--B'}, {'_key': 'ECOM.Payments', '_id': 'Source/ECOM.Payments', '_rev': '_jSQBW3i--D'}]\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3mThe node \"ECOM.v_PaymentsWithOrderInfo\" has two parent nodes. These parent nodes are \"ECOM.Orders\" and \"ECOM.Payments\".\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mNow that we have identified the parent tables of `ECOM.v_PaymentsWithOrderInfo` as `ECOM.Orders` and `ECOM.Payments`, we need to find the upstream dependencies for each of these tables.\n",
            "\n",
            "Let's start by finding the upstream dependencies for `ECOM.Orders` and then for `ECOM.Payments`.\n",
            "\n",
            "Action: get_source_tree\n",
            "Action Input: ECOM.Orders\u001b[0m1) Generating NetworkX code\n",
            "----------\n",
            "import networkx as nx\n",
            "\n",
            "# Step 1: Print trying get_source_tree\n",
            "print(\"trying get_source_tree\")\n",
            "\n",
            "# Step 2: Get the name of the SQL object from the query\n",
            "sql_object_name = \"ECOM.Orders\"\n",
            "\n",
            "# Step 3: Find the node from the graph that has the same or similar name\n",
            "target_node = None\n",
            "for node, data in G_adb.nodes(data=True):\n",
            "    if data.get('_key') == sql_object_name:\n",
            "        target_node = node\n",
            "        break\n",
            "\n",
            "# Step 4: Create a NetworkX DiGraph using the nx_graph.add_edge_from() function\n",
            "nx_graph = nx.DiGraph()  # Create an empty NetworkX DiGraph\n",
            "for source, target, data in G_adb.edges(data=True):\n",
            "    nx_graph.add_edge(source, target)\n",
            "\n",
            "# Step 5: Find all the nodes that are reachable from this node in the graph\n",
            "if target_node is not None:\n",
            "    ancestors = list(nx.ancestors(nx_graph, target_node))\n",
            "else:\n",
            "    ancestors = []\n",
            "\n",
            "# Set the final result\n",
            "FINAL_RESULT = ancestors\n",
            "----------\n",
            "\n",
            "2) Executing NetworkX code\n",
            "trying get_source_tree\n",
            "----------\n",
            "FINAL_RESULT: []\n",
            "----------\n",
            "3) Formulating final answer\n",
            "\n",
            "\n",
            "Observation:\u001b[38;5;200m\u001b[1;3mIf the `FINAL_RESULT` is an empty list, it indicates that there are no SQL objects providing data to \"ECOM.Orders\" in the current graph structure. This could mean that \"ECOM.Orders\" is either not present as a node in the graph or it does not have any data sources connected to it. Therefore, there are no SQL objects to list as providing data to \"ECOM.Orders\".\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe `ECOM.Orders` table does not have any upstream dependencies, meaning it either stands alone in the current graph structure or its data sources are not represented in the graph.\n",
            "\n",
            "Next, let's find the upstream dependencies for the `ECOM.Payments` table.\n",
            "\n",
            "Action: get_source_tree\n",
            "Action Input: ECOM.Payments\u001b[0m1) Generating NetworkX code\n",
            "----------\n",
            "import networkx as nx\n",
            "\n",
            "# Step 1: Print trying get_source_tree\n",
            "print(\"trying get_source_tree\")\n",
            "\n",
            "# Step 2: Get the name of the SQL object from the query\n",
            "sql_object_name = \"ECOM.Payments\"\n",
            "\n",
            "# Step 3: Find the node from the graph that has the same or similar name\n",
            "target_node = None\n",
            "for node, data in G_adb.nodes(data=True):\n",
            "    if data.get('_key') == sql_object_name:\n",
            "        target_node = node\n",
            "        break\n",
            "\n",
            "# Step 4: Create a NetworkX DiGraph using the nx_graph.add_edge_from() function\n",
            "nx_graph = nx.DiGraph()  # Create an empty NetworkX DiGraph\n",
            "for source, target, data in G_adb.edges(data=True):\n",
            "    nx_graph.add_edge(source, target)\n",
            "\n",
            "# Step 5: Find all the nodes that are reachable from this node in the graph\n",
            "if target_node is not None:\n",
            "    ancestors = list(nx.ancestors(nx_graph, target_node))\n",
            "else:\n",
            "    ancestors = []\n",
            "\n",
            "# Set the final result\n",
            "FINAL_RESULT = ancestors\n",
            "----------\n",
            "\n",
            "2) Executing NetworkX code\n",
            "trying get_source_tree\n",
            "----------\n",
            "FINAL_RESULT: []\n",
            "----------\n",
            "3) Formulating final answer\n",
            "\n",
            "\n",
            "Observation:\u001b[38;5;200m\u001b[1;3mBased on the information provided, it seems that the SQL object \"ECOM.Payments\" does not exist in the graph, resulting in an empty list of ancestors. Therefore, there are no SQL objects providing data to \"ECOM.Payments\" in the graph. If \"ECOM.Payments\" were present in the graph, the list of SQL objects providing data to it would be its ancestors in the graph. However, since the node is not found, there are no such objects to list.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe `ECOM.Payments` table also does not have any upstream dependencies in the current graph structure, similar to `ECOM.Orders`. This indicates that either these tables are standalone in the graph or their data sources are not represented in the graph.\n",
            "\n",
            "Therefore, the upstream dependencies of the parent tables of `ECOM.v_PaymentsWithOrderInfo` are not available in the current graph structure. Both `ECOM.Orders` and `ECOM.Payments` do not have any upstream dependencies listed.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The `ECOM.Payments` table also does not have any upstream dependencies in the current graph structure, similar to `ECOM.Orders`. This indicates that either these tables are standalone in the graph or their data sources are not represented in the graph.\\n\\nTherefore, the upstream dependencies of the parent tables of `ECOM.v_PaymentsWithOrderInfo` are not available in the current graph structure. Both `ECOM.Orders` and `ECOM.Payments` do not have any upstream dependencies listed.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Query data from documents hosted on ArangoDB\n",
        "agent_executor.run(\"If there is a refresh delay of 2 hrs in ECOM.Addresses. At what time will it be refreshed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "asH4OZmNO0Aj",
        "outputId": "eef61b52-45a6-4859-ff30-11aff99a077c"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo determine the new refresh time for `ECOM.Addresses` after a 2-hour delay, we first need to find its original refresh time. Let's proceed with that.\n",
            "\n",
            "Thought: I need to find the original refresh time of the `ECOM.Addresses` table.\n",
            "Action: get_table_refreshTime\n",
            "Action Input: ECOM.Addresses\u001b[0mECOM.Addresses\n",
            "\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3m2:05 PM\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe original refresh time for `ECOM.Addresses` is 2:05 PM. With a 2-hour delay, the new refresh time will be 4:05 PM. \n",
            "\n",
            "Final Answer: The `ECOM.Addresses` table will be refreshed at 4:05 PM after a 2-hour delay.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The `ECOM.Addresses` table will be refreshed at 4:05 PM after a 2-hour delay.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use NetworkX to get the downstream dependencies & finally use refresh table calculations to get the new scheduled times\n",
        "agent_executor.run(\"\"\"If there is a refresh delay of 5 hrs in ECOM.Users. Find the new refresh time.\n",
        "If a tables's scheduled run is lesser than the the refresh time of the delayed table then reschedule it so it runs after the delayed table has refreshed.\n",
        "Present your answer as a table with all of the dependent objects of the delayed table names on the left & the final refresh times on the right.\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SP8zG5guQjvV",
        "outputId": "8ba7b08d-7708-4d4c-92e2-278f211d82e6"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mTo address this question, I need to perform the following steps:\n",
            "\n",
            "1. Determine the current refresh time of the `ECOM.Users` table.\n",
            "2. Calculate the new refresh time by adding a 5-hour delay to the current refresh time.\n",
            "3. Identify all downstream dependencies of the `ECOM.Users` table.\n",
            "4. For each dependent object, check its scheduled refresh time and adjust it if it is earlier than the new refresh time of `ECOM.Users`.\n",
            "\n",
            "Let's start by finding the current refresh time of the `ECOM.Users` table.\n",
            "\n",
            "Thought: I need to find the current refresh time of the `ECOM.Users` table.\n",
            "Action: get_table_refreshTime\n",
            "Action Input: ECOM.Users\u001b[0mECOM.Users\n",
            "\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3m4:55 AM\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe current refresh time of the `ECOM.Users` table is 4:55 AM. With a 5-hour delay, the new refresh time will be 9:55 AM. \n",
            "\n",
            "Next, I need to identify all downstream dependencies of the `ECOM.Users` table to determine which tables need their refresh times adjusted.\n",
            "\n",
            "Action: get_sink_tree\n",
            "Action Input: ECOM.Users\u001b[0m1) Generating NetworkX code\n",
            "----------\n",
            "import networkx as nx\n",
            "\n",
            "# Step 1: Print trying get_sink_tree\n",
            "print(\"trying get_sink_tree\")\n",
            "\n",
            "# Step 2: Get the name of the SQL object from the query\n",
            "sql_object_name = \"ECOM.Users\"\n",
            "\n",
            "# Step 3: Find the node from the graph that has the same or similar name\n",
            "target_node = None\n",
            "for node, data in G_adb.nodes(data=True):\n",
            "    if data.get('_key') == sql_object_name:\n",
            "        target_node = node\n",
            "        break\n",
            "\n",
            "# Step 4: Convert ArangoDB graph to NetworkX graph\n",
            "nx_graph = nx.DiGraph()  # Create an empty NetworkX DiGraph\n",
            "for source, target, data in G_adb.edges(data=True):\n",
            "    nx_graph.add_edge(source, target)\n",
            "\n",
            "# Step 5: Find all the nodes that are reachable from this node in the graph\n",
            "descendants = set(nx.descendants(nx_graph, target_node))\n",
            "ancestors = set(nx.ancestors(nx_graph, target_node))\n",
            "sinks = list(descendants - ancestors)\n",
            "\n",
            "# Step 6: Analyse the answer & only return the SQL objects that are consuming data\n",
            "FINAL_RESULT = [G_adb.nodes[node]['_key'] for node in sinks if '_key' in G_adb.nodes[node]]\n",
            "----------\n",
            "\n",
            "2) Executing NetworkX code\n",
            "trying get_sink_tree\n",
            "----------\n",
            "FINAL_RESULT: ['ECOM.v_OrdersWithUserInfo', 'ECOM.v_UserFullName', 'ECOM.v_UserGenderBirthdate', 'ECOM.v_ReviewsWithProductUserInfo', 'ECOM.v_WishlistWithUserProductInfo', 'ECOM.v_UserBasicInfo']\n",
            "----------\n",
            "3) Formulating final answer\n",
            "\n",
            "\n",
            "Observation:\u001b[33;1m\u001b[1;3mBased on the information provided, the SQL objects that are consuming data from \"ECOM.Users\" are:\n",
            "\n",
            "1. ECOM.v_OrdersWithUserInfo\n",
            "2. ECOM.v_UserFullName\n",
            "3. ECOM.v_UserGenderBirthdate\n",
            "4. ECOM.v_ReviewsWithProductUserInfo\n",
            "5. ECOM.v_WishlistWithUserProductInfo\n",
            "6. ECOM.v_UserBasicInfo\n",
            "\n",
            "These are the SQL objects that are directly consuming data from the \"ECOM.Users\" table.\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI have identified the downstream dependencies of the `ECOM.Users` table. These are the SQL objects that consume data from it:\n",
            "\n",
            "1. ECOM.v_OrdersWithUserInfo\n",
            "2. ECOM.v_UserFullName\n",
            "3. ECOM.v_UserGenderBirthdate\n",
            "4. ECOM.v_ReviewsWithProductUserInfo\n",
            "5. ECOM.v_WishlistWithUserProductInfo\n",
            "6. ECOM.v_UserBasicInfo\n",
            "\n",
            "Now, I need to check the scheduled refresh times for each of these dependent objects and adjust them if they are earlier than the new refresh time of 9:55 AM.\n",
            "\n",
            "Let's proceed with checking and adjusting the refresh times for these dependent objects. I'll start by finding the refresh times for each of them. \n",
            "\n",
            "Action: get_table_refreshTime\n",
            "Action Input: ECOM.v_OrdersWithUserInfo\u001b[0mECOM.v_OrdersWithUserInfo\n",
            "\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3m6:01 PM\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe current refresh time for `ECOM.v_OrdersWithUserInfo` is 6:01 PM, which is later than the new refresh time of `ECOM.Users` at 9:55 AM. Therefore, no adjustment is needed for this object.\n",
            "\n",
            "Next, I will check the refresh time for the next dependent object, `ECOM.v_UserFullName`.\n",
            "\n",
            "Action: get_table_refreshTime\n",
            "Action Input: ECOM.v_UserFullName\u001b[0mECOM.v_UserFullName\n",
            "\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3m2:26 AM\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe current refresh time for `ECOM.v_UserFullName` is 2:26 AM, which is earlier than the new refresh time of `ECOM.Users` at 9:55 AM. Therefore, the refresh time for `ECOM.v_UserFullName` needs to be adjusted to occur after 9:55 AM.\n",
            "\n",
            "Next, I will check the refresh time for the next dependent object, `ECOM.v_UserGenderBirthdate`.\n",
            "\n",
            "Action: get_table_refreshTime\n",
            "Action Input: ECOM.v_UserGenderBirthdate\u001b[0mECOM.v_UserGenderBirthdate\n",
            "\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3m8:23 AM\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe current refresh time for `ECOM.v_UserGenderBirthdate` is 8:23 AM, which is earlier than the new refresh time of `ECOM.Users` at 9:55 AM. Therefore, the refresh time for `ECOM.v_UserGenderBirthdate` needs to be adjusted to occur after 9:55 AM.\n",
            "\n",
            "Next, I will check the refresh time for the next dependent object, `ECOM.v_ReviewsWithProductUserInfo`.\n",
            "\n",
            "Action: get_table_refreshTime\n",
            "Action Input: ECOM.v_ReviewsWithProductUserInfo\u001b[0mECOM.v_ReviewsWithProductUserInfo\n",
            "\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3m12:08 AM\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe current refresh time for `ECOM.v_ReviewsWithProductUserInfo` is 12:08 AM, which is earlier than the new refresh time of `ECOM.Users` at 9:55 AM. Therefore, the refresh time for `ECOM.v_ReviewsWithProductUserInfo` needs to be adjusted to occur after 9:55 AM.\n",
            "\n",
            "Next, I will check the refresh time for the next dependent object, `ECOM.v_WishlistWithUserProductInfo`.\n",
            "\n",
            "Action: get_table_refreshTime\n",
            "Action Input: ECOM.v_WishlistWithUserProductInfo\u001b[0mECOM.v_WishlistWithUserProductInfo\n",
            "\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3m6:35 AM\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe current refresh time for `ECOM.v_WishlistWithUserProductInfo` is 6:35 AM, which is earlier than the new refresh time of `ECOM.Users` at 9:55 AM. Therefore, the refresh time for `ECOM.v_WishlistWithUserProductInfo` needs to be adjusted to occur after 9:55 AM.\n",
            "\n",
            "Finally, I will check the refresh time for the last dependent object, `ECOM.v_UserBasicInfo`.\n",
            "\n",
            "Action: get_table_refreshTime\n",
            "Action Input: ECOM.v_UserBasicInfo\u001b[0mECOM.v_UserBasicInfo\n",
            "\n",
            "\n",
            "Observation:\u001b[36;1m\u001b[1;3m10:34 AM\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe current refresh time for `ECOM.v_UserBasicInfo` is 10:34 AM, which is later than the new refresh time of `ECOM.Users` at 9:55 AM. Therefore, no adjustment is needed for this object.\n",
            "\n",
            "Now, I will summarize the adjustments needed for the dependent objects:\n",
            "\n",
            "| Dependent Object                      | Final Refresh Time |\n",
            "|---------------------------------------|--------------------|\n",
            "| ECOM.v_OrdersWithUserInfo             | 6:01 PM            |\n",
            "| ECOM.v_UserFullName                   | 9:55 AM            |\n",
            "| ECOM.v_UserGenderBirthdate            | 9:55 AM            |\n",
            "| ECOM.v_ReviewsWithProductUserInfo     | 9:55 AM            |\n",
            "| ECOM.v_WishlistWithUserProductInfo    | 9:55 AM            |\n",
            "| ECOM.v_UserBasicInfo                  | 10:34 AM           |\n",
            "\n",
            "The refresh times for `ECOM.v_UserFullName`, `ECOM.v_UserGenderBirthdate`, `ECOM.v_ReviewsWithProductUserInfo`, and `ECOM.v_WishlistWithUserProductInfo` need to be adjusted to occur after the new refresh time of `ECOM.Users` at 9:55 AM. The other objects do not require any changes.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current refresh time for `ECOM.v_UserBasicInfo` is 10:34 AM, which is later than the new refresh time of `ECOM.Users` at 9:55 AM. Therefore, no adjustment is needed for this object.\\n\\nNow, I will summarize the adjustments needed for the dependent objects:\\n\\n| Dependent Object                      | Final Refresh Time |\\n|---------------------------------------|--------------------|\\n| ECOM.v_OrdersWithUserInfo             | 6:01 PM            |\\n| ECOM.v_UserFullName                   | 9:55 AM            |\\n| ECOM.v_UserGenderBirthdate            | 9:55 AM            |\\n| ECOM.v_ReviewsWithProductUserInfo     | 9:55 AM            |\\n| ECOM.v_WishlistWithUserProductInfo    | 9:55 AM            |\\n| ECOM.v_UserBasicInfo                  | 10:34 AM           |\\n\\nThe refresh times for `ECOM.v_UserFullName`, `ECOM.v_UserGenderBirthdate`, `ECOM.v_ReviewsWithProductUserInfo`, and `ECOM.v_WishlistWithUserProductInfo` need to be adjusted to occur after the new refresh time of `ECOM.Users` at 9:55 AM. The other objects do not require any changes.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    }
  ]
}